{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec416ff9",
   "metadata": {},
   "source": [
    "# Transformer from Scratch - Elias Hossain  \n",
    "### Graduate Student at Mississippi State University  \n",
    "\n",
    "This notebook implements a Transformer model from scratch, inspired by the **\"Attention Is All You Need\"** paper.  \n",
    "We will follow these key steps:\n",
    "\n",
    "1. **Prepare Dummy Data**\n",
    "2. **Tokenization & Embeddings**\n",
    "3. **Positional Encoding**\n",
    "4. **Multi-Head Self-Attention**\n",
    "5. **Feed-Forward Network**\n",
    "6. **Stack Transformer Blocks**\n",
    "7. **Training with Loss and Accuracy Computation**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025c763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b918e6c9",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1acdfb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mh3511\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23419a41",
   "metadata": {},
   "source": [
    "### Step 1: Create Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4773dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Data: tensor([[6, 9, 6, 2, 8],\n",
      "        [5, 8, 9, 4, 4]])\n",
      "Target Data: tensor([[6, 6, 3, 9, 6],\n",
      "        [7, 5, 5, 4, 0]])\n"
     ]
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = 10  # Small vocabulary for the source language\n",
    "TGT_VOCAB_SIZE = 10  # Small vocabulary for the target language\n",
    "MAX_LEN = 5  # Max length of input/output sequence\n",
    "EMBED_DIM = 8  # Embedding dimension\n",
    "BATCH_SIZE = 2  # Small batch size\n",
    "\n",
    "# Dummy input (random integers representing words in vocab)\n",
    "source_data = torch.randint(0, SRC_VOCAB_SIZE, (BATCH_SIZE, MAX_LEN))\n",
    "target_data = torch.randint(0, TGT_VOCAB_SIZE, (BATCH_SIZE, MAX_LEN))\n",
    "\n",
    "print(\"Source Data:\", source_data)\n",
    "print(\"Target Data:\", target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbca90",
   "metadata": {},
   "source": [
    "### Step 2: Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27197ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embedding Shape: torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "source_embedding = TokenEmbedding(SRC_VOCAB_SIZE, EMBED_DIM)\n",
    "target_embedding = TokenEmbedding(TGT_VOCAB_SIZE, EMBED_DIM)\n",
    "\n",
    "source_embedded = source_embedding(source_data)\n",
    "target_embedded = target_embedding(target_data)\n",
    "print(\"Token Embedding Shape:\", source_embedded.shape)  # Should be (BATCH_SIZE, MAX_LEN, EMBED_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed156ee",
   "metadata": {},
   "source": [
    "### Step 3: Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9988162a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding Added: torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(np.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "pos_encoder = PositionalEncoding(EMBED_DIM)\n",
    "source_encoded = pos_encoder(source_embedded)\n",
    "target_encoded = pos_encoder(target_embedded)\n",
    "print(\"Positional Encoding Added:\", source_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9e1e2",
   "metadata": {},
   "source": [
    "### Step 4: Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91bd3a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv_proj(x).chunk(3, dim=-1)  # Split into Q, K, V\n",
    "        Q, K, V = [t.view(B, T, self.num_heads, self.head_dim).transpose(1, 2) for t in qkv]\n",
    "        \n",
    "        scores = (Q @ K.transpose(-2, -1)) / self.scale\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = (attn @ V).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba75806",
   "metadata": {},
   "source": [
    "### Step 5: Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d8316a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74688503",
   "metadata": {},
   "source": [
    "### Step 6: Transformer Model with Output Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f060a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForwardNetwork(embed_dim, hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_out = self.attn(x)\n",
    "        x = self.norm1(x + attn_out)  # Residual Connection\n",
    "        ffn_out = self.ffn(x)\n",
    "        return self.norm2(x + ffn_out)  # Residual Connection\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim, num_heads, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.src_embed = TokenEmbedding(src_vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "        self.encoder = nn.ModuleList([TransformerBlock(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(embed_dim, tgt_vocab_size)  # Output projection layer\n",
    "    \n",
    "    def forward(self, src):\n",
    "        src = self.pos_enc(self.src_embed(src))\n",
    "        for layer in self.encoder:\n",
    "            src = layer(src)\n",
    "        return self.fc_out(src)  # Project to target vocab size\n",
    "\n",
    "transformer = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, EMBED_DIM, num_heads=2, hidden_dim=32, num_layers=2)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6099bd",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "462848a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.8160, Accuracy: 0.00%\n",
      "Epoch 2/10, Loss: 2.7852, Accuracy: 0.00%\n",
      "Epoch 3/10, Loss: 2.5829, Accuracy: 0.00%\n",
      "Epoch 4/10, Loss: 2.6864, Accuracy: 0.00%\n",
      "Epoch 5/10, Loss: 2.1746, Accuracy: 20.00%\n",
      "Epoch 6/10, Loss: 2.5760, Accuracy: 10.00%\n",
      "Epoch 7/10, Loss: 2.3655, Accuracy: 20.00%\n",
      "Epoch 8/10, Loss: 2.9434, Accuracy: 0.00%\n",
      "Epoch 9/10, Loss: 2.7594, Accuracy: 10.00%\n",
      "Epoch 10/10, Loss: 2.8197, Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    output = transformer(source_data)\n",
    "    target_labels = torch.randint(0, TGT_VOCAB_SIZE, (BATCH_SIZE, MAX_LEN))\n",
    "    loss = criterion(output.view(-1, TGT_VOCAB_SIZE), target_labels.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predictions = torch.argmax(output, dim=-1)\n",
    "    correct = (predictions == target_labels).sum().item()\n",
    "    total = target_labels.numel()\n",
    "    accuracy = correct / total * 100\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d668a16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
