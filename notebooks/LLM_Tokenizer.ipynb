{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0b0486",
   "metadata": {},
   "source": [
    "# üöÄ 60 Days of LLM Development from Scratch  \n",
    "\n",
    "### Day 3: Tokenizer Implementation in LLMs  \n",
    "\n",
    "In our **60-day journey** of **LLM development from scratch**, we explore the fundamental components of building a Large Language Model.  \n",
    "\n",
    "Today, on **Day 3**, we focus on **tokenization**, a critical step in transforming raw text into a format that LLMs can process efficiently. We implement **Byte Pair Encoding (BPE)** and explore **Token IDs, Token Embeddings, and Positional Embeddings**, essential for understanding how transformers handle textual data.\n",
    "\n",
    "üîπ **Author:** **Elias Hossain**  \n",
    "üîπ **Affiliation:** **Graduate Student, Computer Science, Mississippi State University**  \n",
    "\n",
    "Stay tuned for more updates as we dive deeper into LLM development! üöÄ  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3b127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce1cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d9f5855",
   "metadata": {},
   "source": [
    "### STEP 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6793b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ransformers (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ransformers (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ransformers (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abdd3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a075c",
   "metadata": {},
   "source": [
    "### STEP 2: Function to Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b380d680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "# PDF file location\n",
    "pdf_path = r\"C:\\Users\\mh3511\\Desktop\\LLM Development\\data\\historybrief.pdf\"\n",
    "output_text_file = \"extracted_text.txt\"\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Save extracted text\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "with open(output_text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(extracted_text)\n",
    "\n",
    "print(\"Text extracted and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd80fe",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------\n",
    "# Byte-Pair Encoding (BPE) Tokenizer\n",
    "# ----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08287e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE tokenization done.\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = Tokenizer(models.BPE())\n",
    "bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "bpe_tokenizer.train([output_text_file], trainer)\n",
    "bpe_tokenizer.save(\"bpe_tokenizer.json\")\n",
    "print(\"BPE tokenization done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76bc724",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# WordPiece Tokenizer\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f79bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece tokenization done.\n"
     ]
    }
   ],
   "source": [
    "wp_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "wp_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.WordPieceTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "wp_tokenizer.train([output_text_file], trainer)\n",
    "wp_tokenizer.save(\"wordpiece_tokenizer.json\")\n",
    "print(\"WordPiece tokenization done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba292f",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------\n",
    "# Unigram Tokenizer (SentencePiece)\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd620069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tokenization done.\n"
     ]
    }
   ],
   "source": [
    "unigram_model_prefix = \"unigram_tokenizer\"\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=output_text_file, model_prefix=unigram_model_prefix, vocab_size=2600, model_type=\"unigram\"\n",
    ")\n",
    "print(\"Unigram tokenization done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe11696",
   "metadata": {},
   "source": [
    "# ---------------------------------------------\n",
    "# SentencePiece BPE Tokenizer\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb44cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece BPE tokenization done.\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input=output_text_file, model_prefix=\"sentencepiece_bpe\", vocab_size=5000, model_type=\"bpe\"\n",
    ")\n",
    "print(\"SentencePiece BPE tokenization done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f721fc",
   "metadata": {},
   "source": [
    "# ------------------------------------------\n",
    "# Character-Level Tokenization\n",
    "# ------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8a5fd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level tokenization done.\n",
      "All tokenization processes completed. Models saved.\n"
     ]
    }
   ],
   "source": [
    "char_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "char_tokenizer.pre_tokenizer = pre_tokenizers.Split(\"\", \"isolated\")  # Character-level splitting\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "char_tokenizer.train([output_text_file], trainer)\n",
    "char_tokenizer.save(\"char_tokenizer.json\")\n",
    "print(\"Character-level tokenization done.\")\n",
    "\n",
    "print(\"All tokenization processes completed. Models saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88cf3c7",
   "metadata": {},
   "source": [
    "What we have done so far?\n",
    "\n",
    "1) Extracts text from your historybrief.pdf file.\n",
    "2) Implements BPE, WordPiece, Unigram, SentencePiece (BPE & Unigram), and Character-Level Tokenization.\n",
    "3) Saves the trained tokenizers as JSON and SentencePiece model files.\n",
    "4) Outputs progress updates in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f09b3",
   "metadata": {},
   "source": [
    "What is our Next Steps?\n",
    "1) We can use the trained tokenizers to tokenize any text by loading them using the tokenizers library.\n",
    "2) To tokenize a sentence, we can use the above tokenizer that we created and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce27ca77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'amp', 'le', 't', 'ext', 'for', 'to', 'k', 'en', 'ization']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")  # Load desired tokenizer\n",
    "print(tokenizer.encode(\"Sample text for tokenization\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118756a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9724962",
   "metadata": {},
   "source": [
    "The above code is a simple implementation about the different tokenizer used by the LLMs. However, in the following, I will create a full pipeline which is more sophisticated and end-to-end pipeline. For this reason, I took BPE tokenizer, you can use diffrent one as core process is similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab637efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mh3511\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text extracted and saved.\n",
      "‚úÖ Text preprocessed and saved.\n",
      "‚úÖ BPE Tokenizer trained and saved.\n",
      "üîπ Tokens: ['[UNK]', 'ee', 'p', 'lear', 'ning', 'is', 'revolution', 'izing', 'art', 'if', 'icial', 'inte', 'lli', 'gence', '[UNK]']\n",
      "üîπ Token IDs: [1, 613, 30, 3266, 1062, 65, 462, 2203, 521, 1242, 3218, 1986, 2871, 1896, 1]\n",
      "‚úÖ Token Embeddings Shape: torch.Size([15, 128])\n",
      "‚úÖ Positional Encoding Applied. Final Shape: torch.Size([1, 15, 128])\n",
      "\n",
      "üìù Final Tokenized Output:\n",
      "1. Token: [UNK] | Token ID: 1 | Embedding: [-0.10346663743257523, 0.2682356834411621, 0.925059974193573, 2.831432342529297, -1.8659424781799316] ...\n",
      "2. Token: ee | Token ID: 613 | Embedding: [-0.3140270709991455, 1.70341157913208, 1.835204839706421, -0.11333292722702026, 0.3725041449069977] ...\n",
      "3. Token: p | Token ID: 30 | Embedding: [0.33506327867507935, -1.8702374696731567, 2.0708506107330322, -0.4936813712120056, -0.02078145742416382] ...\n",
      "4. Token: lear | Token ID: 3266 | Embedding: [0.7550011873245239, -1.6172904968261719, -1.1937506198883057, -0.2293456792831421, 0.5647960901260376] ...\n",
      "5. Token: ning | Token ID: 1062 | Embedding: [-0.3943213224411011, -0.5335462689399719, 0.1347343623638153, 0.9592500925064087, 0.792007327079773] ...\n",
      "6. Token: is | Token ID: 65 | Embedding: [-1.2081493139266968, -0.049453020095825195, -2.9761784076690674, -0.9636352062225342, -1.8797836303710938] ...\n",
      "7. Token: revolution | Token ID: 462 | Embedding: [0.45048338174819946, 2.8425955772399902, -1.5749092102050781, 1.87872314453125, -1.0879321098327637] ...\n",
      "8. Token: izing | Token ID: 2203 | Embedding: [2.8106441497802734, 1.383589267730713, 0.3157612085342407, -0.9021605253219604, -1.277878999710083] ...\n",
      "9. Token: art | Token ID: 521 | Embedding: [-0.4772549271583557, -1.134088158607483, 1.7655029296875, 2.087186813354492, 0.20116907358169556] ...\n",
      "10. Token: if | Token ID: 1242 | Embedding: [0.6750596761703491, -1.0730540752410889, 1.5288885831832886, -1.0078938007354736, 1.9163100719451904] ...\n",
      "11. Token: icial | Token ID: 3218 | Embedding: [-0.42667844891548157, -0.1515231728553772, 1.7700234651565552, 0.3169568181037903, 0.3774796724319458] ...\n",
      "12. Token: inte | Token ID: 1986 | Embedding: [-1.1343965530395508, -0.5114418268203735, -1.1217308044433594, -0.9770997762680054, -0.33596664667129517] ...\n",
      "13. Token: lli | Token ID: 2871 | Embedding: [-0.2923334240913391, 0.839887797832489, 0.9041253328323364, 0.1647534966468811, 0.1279062032699585] ...\n",
      "14. Token: gence | Token ID: 1896 | Embedding: [0.11002641916275024, 1.2789766788482666, -2.526279926300049, -0.45546987652778625, -0.2978294789791107] ...\n",
      "15. Token: [UNK] | Token ID: 1 | Embedding: [0.8871407508850098, -0.5950270891189575, 0.49652495980262756, 2.734957456588745, -2.7449328899383545] ...\n",
      "üöÄ Full BPE Tokenizer with Embeddings and Positional Encoding Completed!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
    "\n",
    "# PDF File Path\n",
    "pdf_path = r\"C:\\Users\\mh3511\\Desktop\\LLM Development\\data\\historybrief.pdf\"\n",
    "output_text_file = \"extracted_text.txt\"\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Save extracted text\n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "with open(output_text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(raw_text)\n",
    "\n",
    "print(\"‚úÖ Text extracted and saved.\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 1Ô∏è‚É£ Text Preprocessing\n",
    "# --------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "clean_text = preprocess_text(raw_text)\n",
    "with open(\"clean_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(clean_text)\n",
    "\n",
    "print(\"‚úÖ Text preprocessed and saved.\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 2Ô∏è‚É£ Byte Pair Encoding (BPE) Tokenizer Training\n",
    "# --------------------------------------------\n",
    "bpe_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "bpe_tokenizer.train([\"clean_text.txt\"], trainer)\n",
    "bpe_tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "print(\"‚úÖ BPE Tokenizer trained and saved.\")\n",
    "\n",
    "# Load Trained BPE Tokenizer\n",
    "bpe_tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
    "\n",
    "# Example Sentence for Tokenization\n",
    "sentence = \"Deep learning is revolutionizing artificial intelligence.\"\n",
    "\n",
    "# Tokenizing the Sentence\n",
    "encoded = bpe_tokenizer.encode(sentence)\n",
    "token_ids = encoded.ids\n",
    "tokens = encoded.tokens\n",
    "\n",
    "print(f\"üîπ Tokens: {tokens}\")\n",
    "print(f\"üîπ Token IDs: {token_ids}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3Ô∏è‚É£ Token Embeddings (Learnable Matrix)\n",
    "# --------------------------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "# Define Parameters\n",
    "vocab_size = 5000  # Adjust as needed\n",
    "embedding_dim = 128  # Standard embedding size\n",
    "\n",
    "# Create Token Embedding Model\n",
    "token_embedding = TokenEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Convert Token IDs to Tensor\n",
    "token_ids_tensor = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "# Generate Token Embeddings\n",
    "embedded_tokens = token_embedding(token_ids_tensor)\n",
    "print(f\"‚úÖ Token Embeddings Shape: {embedded_tokens.shape}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4Ô∏è‚É£ Positional Encoding (Sinusoidal)\n",
    "# --------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-np.log(10000.0) / embedding_dim))\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Define Positional Encoding\n",
    "max_seq_length = 512  # Adjust as needed\n",
    "positional_encoding = PositionalEncoding(max_seq_length, embedding_dim)\n",
    "\n",
    "# Apply Positional Encoding\n",
    "embedded_tokens_with_pos = positional_encoding(embedded_tokens.unsqueeze(0))\n",
    "\n",
    "print(f\"‚úÖ Positional Encoding Applied. Final Shape: {embedded_tokens_with_pos.shape}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5Ô∏è‚É£ Full Tokenization Pipeline Output\n",
    "# --------------------------------------------\n",
    "print(\"\\nüìù Final Tokenized Output:\")\n",
    "for i, (tok, tok_id, emb) in enumerate(zip(tokens, token_ids, embedded_tokens_with_pos.squeeze(0))):\n",
    "    print(f\"{i+1}. Token: {tok} | Token ID: {tok_id} | Embedding: {emb.tolist()[:5]} ...\")  # Showing first 5 dims of embeddings\n",
    "\n",
    "print(\"üöÄ Full BPE Tokenizer with Embeddings and Positional Encoding Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf0531",
   "metadata": {},
   "source": [
    "# üîπ Tokenization Components in LLMs\n",
    "\n",
    "## 1Ô∏è‚É£ Token ID\n",
    "- Converts words into unique numbers for processing.\n",
    "- Example: `\"Deep learning\"` ‚Üí `[123, 456]`\n",
    "\n",
    "## 2Ô∏è‚É£ Token Embeddings\n",
    "- Maps tokens to high-dimensional vectors.\n",
    "- Helps the model understand word meanings.\n",
    "- Example: `Token ID 456 ‚Üí [0.12, -0.34, 0.89, ...]`\n",
    "\n",
    "## 3Ô∏è‚É£ Positional Embeddings\n",
    "- Adds position info since transformers process words in parallel.\n",
    "- Example: `Position 1 ‚Üí [0.001, -0.23, 0.45, ...]`\n",
    "\n",
    "## 4Ô∏è‚É£ Special Tokens\n",
    "- `[PAD]` (padding), `[UNK]` (unknown), `[CLS]` (classification), `[SEP]` (separator), `[MASK]` (masking).\n",
    "\n",
    "## 5Ô∏è‚É£ Byte Pair Encoding (BPE)\n",
    "- Splits words into subwords for better handling of rare words.\n",
    "- Example: `\"unhappiness\"` ‚Üí `[\"un\", \"happiness\"]`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
