{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# I'm loading my preprocessing data from my repository \n",
    "url = \"https://raw.githubusercontent.com/eliashossain001/LLM-Development/main/outputs/clean_text.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 2957\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# Fetch the content\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    raw_text = response.text\n",
    "\n",
    "    # Tokenization: Extract words, ignoring punctuation\n",
    "    tokenized_text = re.findall(r\"\\b\\w+\\b\", raw_text)\n",
    "\n",
    "    # Extract unique words and sort them\n",
    "    distinct_tokens = sorted(set(tokenized_text))\n",
    "\n",
    "    # Get the vocabulary size\n",
    "    total_unique_words = len(distinct_tokens)\n",
    "\n",
    "    # Display results\n",
    "    print(\"Total unique words:\", total_unique_words)\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to fetch the file. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2957\n"
     ]
    }
   ],
   "source": [
    "distinct_tokens = sorted(set(tokenized_text))\n",
    "total_unique_words = len(distinct_tokens)\n",
    "\n",
    "print(total_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2957\n",
      "First 10 token IDs: [1374, 1450, 507, 1244, 2869, 241, 2670, 737, 755, 1450]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "# Fetch the content\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    raw_text = response.text\n",
    "\n",
    "    # Tokenization: Extract words, ignoring punctuation\n",
    "    tokenized_text = re.findall(r\"\\b\\w+\\b\", raw_text)\n",
    "\n",
    "    # Get unique words and sort them\n",
    "    distinct_tokens = sorted(set(tokenized_text))\n",
    "    total_unique_words = len(distinct_tokens)\n",
    "\n",
    "    # Create token-to-ID mapping\n",
    "    token_to_id = {word: idx for idx, word in enumerate(distinct_tokens)}\n",
    "\n",
    "    # Convert tokenized text into token IDs\n",
    "    token_ids = [token_to_id[word] for word in tokenized_text]\n",
    "\n",
    "    # Display results\n",
    "    print(\"Vocabulary size:\", total_unique_words)\n",
    "    print(\"First 10 token IDs:\", token_ids[:10])  # Preview first 10 token IDs\n",
    "else:\n",
    "    print(\"Failed to fetch the file. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 token mappings: [('000', 0), ('1', 1), ('10', 2), ('100', 3), ('100000', 4), ('10001550', 5), ('1001', 6), ('10relations', 7), ('11', 8), ('11road', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary mapping each unique token to an integer ID\n",
    "vocab = {token: integer for integer, token in enumerate(distinct_tokens)}\n",
    "\n",
    "# Display the first 10 token mappings as an example\n",
    "print(\"First 10 token mappings:\", list(vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('000', 0)\n",
      "('1', 1)\n",
      "('10', 2)\n",
      "('100', 3)\n",
      "('100000', 4)\n",
      "('10001550', 5)\n",
      "('1001', 6)\n",
      "('10relations', 7)\n",
      "('11', 8)\n",
      "('11road', 9)\n",
      "('12000', 10)\n",
      "('12stamps', 11)\n",
      "('13', 12)\n",
      "('130', 13)\n",
      "('1330foot', 14)\n",
      "('13th', 15)\n",
      "('13to', 16)\n",
      "('1415', 17)\n",
      "('1492', 18)\n",
      "('14point', 19)\n",
      "('14the', 20)\n",
      "('15', 21)\n",
      "('1500', 22)\n",
      "('1500s', 23)\n",
      "('1513', 24)\n",
      "('1522', 25)\n",
      "('1539', 26)\n",
      "('1540', 27)\n",
      "('1600s', 28)\n",
      "('1617', 29)\n",
      "('1620', 30)\n",
      "('16721695', 31)\n",
      "('168889', 32)\n",
      "('1690', 33)\n",
      "('16artists', 34)\n",
      "('17', 35)\n",
      "('170', 36)\n",
      "('17000', 37)\n",
      "('1700s', 38)\n",
      "('17381820', 39)\n",
      "('1750000', 40)\n",
      "('1750s', 41)\n",
      "('1763', 42)\n",
      "('1764', 43)\n",
      "('1765', 44)\n",
      "('1770s', 45)\n",
      "('1773', 46)\n",
      "('1774', 47)\n",
      "('1775', 48)\n",
      "('1776', 49)\n",
      "('1778', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class BasicTextTokenizer:\n",
    "    def __init__(self, token_dict):\n",
    "        # Mapping words to unique IDs\n",
    "        self.token_to_id = token_dict\n",
    "        # Reverse mapping: IDs back to words\n",
    "        self.id_to_token = {idx: token for token, idx in token_dict.items()}\n",
    "\n",
    "    def encode(self, input_text):\n",
    "        # Tokenize text by splitting based on punctuation and whitespace\n",
    "        token_list = re.split(r'([,.:;?_!\"()\\']|--|\\s)', input_text)\n",
    "\n",
    "        # Remove empty tokens and extra spaces\n",
    "        token_list = [token.strip() for token in token_list if token.strip()]\n",
    "\n",
    "        # Convert tokens into their corresponding IDs\n",
    "        token_ids = [self.token_to_id[token] for token in token_list]\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        # Convert token IDs back to words\n",
    "        reconstructed_text = \" \".join([self.id_to_token[idx] for idx in token_ids])\n",
    "\n",
    "        # Remove unnecessary spaces before punctuation marks\n",
    "        reconstructed_text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', reconstructed_text)\n",
    "\n",
    "        return reconstructed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "class BasicWordTokenizer:\n",
    "    def __init__(self, vocabulary):\n",
    "        # Store vocabulary\n",
    "        self.word_list = vocabulary\n",
    "        # Create a mapping of words to unique IDs\n",
    "        self.word_to_id = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "        # Define an unknown token placeholder\n",
    "        self.unknown_token = \"<|unk|>\"\n",
    "        # Assign an ID for unknown words (out of vocabulary)\n",
    "        self.unknown_id = len(vocabulary)\n",
    "\n",
    "    def encode(self, input_text):\n",
    "        # Split text into tokens using punctuation and whitespace as delimiters\n",
    "        tokenized_words = re.findall(r\"\\b\\w+\\b\", input_text)\n",
    "        # Convert tokens to their respective IDs, using the unknown ID for out-of-vocabulary words\n",
    "        token_ids = [self.word_to_id.get(word, self.unknown_id) for word in tokenized_words]\n",
    "        return tokenized_words, token_ids  # Returning tokenized words and token IDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
    "from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size before adding special tokens: 2957\n",
      "First 10 tokens: [('000', 0), ('1', 1), ('10', 2), ('100', 3), ('100000', 4), ('10001550', 5), ('1001', 6), ('10relations', 7), ('11', 8), ('11road', 9)]\n",
      "Vocabulary size after adding special tokens: 2959\n",
      "Special tokens added: ['<|endoftext|>', '<|unk|>']\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    raw_text = response.text\n",
    "\n",
    "    # Tokenize text\n",
    "    tokenized_words, _ = BasicWordTokenizer([]).encode(raw_text)  # Extract tokens without a predefined vocab\n",
    "\n",
    "    # Extract unique words and sort them\n",
    "    distinct_tokens = sorted(list(set(tokenized_words)))\n",
    "\n",
    "    # Create a vocabulary mapping from tokens to integer IDs\n",
    "    vocab = {token: idx for idx, token in enumerate(distinct_tokens)}\n",
    "\n",
    "    print(\"Vocabulary size before adding special tokens:\", len(vocab))\n",
    "    print(\"First 10 tokens:\", list(vocab.items())[:10])\n",
    "\n",
    "    # **Separate Section: Adding Special Tokens**\n",
    "    special_tokens = [\"<|endoftext|>\", \"<|unk|>\"]\n",
    "    for special_token in special_tokens:\n",
    "        vocab[special_token] = len(vocab)  # Assign a new ID\n",
    "\n",
    "    print(\"Vocabulary size after adding special tokens:\", len(vocab))\n",
    "    print(\"Special tokens added:\", special_tokens)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to fetch the file. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2959"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('youth', 2954)\n",
      "('zedong', 2955)\n",
      "('zuni', 2956)\n",
      "('<|endoftext|>', 2957)\n",
      "('<|unk|>', 2958)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class CustomTextTokenizer:\n",
    "    def __init__(self, token_mapping):\n",
    "        # Store mappings of tokens to unique IDs\n",
    "        self.token_to_id = token_mapping\n",
    "        # Reverse mapping: IDs back to tokens\n",
    "        self.id_to_token = {idx: token for token, idx in token_mapping.items()}\n",
    "\n",
    "    def encode(self, input_text):\n",
    "        # Tokenize text using punctuation and whitespace as delimiters\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', input_text)\n",
    "        # Remove empty and whitespace-only tokens\n",
    "        tokens = [token.strip() for token in tokens if token.strip()]\n",
    "        # Replace unknown tokens with a placeholder \"<|unk|>\"\n",
    "        tokens = [token if token in self.token_to_id else \"<|unk|>\" for token in tokens]\n",
    "\n",
    "        # Convert tokens to their corresponding IDs\n",
    "        token_ids = [self.token_to_id[token] for token in tokens]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        # Convert token IDs back to tokens\n",
    "        reconstructed_text = \" \".join([self.id_to_token[idx] for idx in token_ids])\n",
    "        # Remove unnecessary spaces before punctuation marks\n",
    "        reconstructed_text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', reconstructed_text)\n",
    "        return reconstructed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Elias Hossain <|endoftext|> I love Pizza and Music\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomTextTokenizer(vocab)\n",
    "\n",
    "text1 = \"Hello, my name is Elias Hossain\"\n",
    "text2 = \"I love Pizza and Music\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2958, 2958, 2958, 1819, 1522, 2958, 2958, 2957, 2958, 2958, 2958, 326, 2958]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> <|unk|> <|unk|> name is <|unk|> <|unk|> <|endoftext|> <|unk|> <|unk|> <|unk|> and <|unk|>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BYTE PAIR ENCODING (BPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\mh3511\\appdata\\local\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\mh3511\\appdata\\local\\anaconda3\\lib\\site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\mh3511\\appdata\\local\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mh3511\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mh3511\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mh3511\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mh3511\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ransformers (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ransformers (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ransformers (c:\\users\\mh3511\\appdata\\local\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib_metadata \n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 644, 318, 534, 717, 1438, 30, 220, 50256, 345, 389, 407, 922, 379, 477, 1659, 10688, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, what is your first name? <|endoftext|> you are not good at all\"\n",
    "     \"of math.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, what is your first name? <|endoftext|> you are not good at allof math.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING INPUT-TARGET PAIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of encoded tokens: 12320\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Retrieve the file content\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    text_data = response.text  # Extract the text from the response\n",
    "\n",
    "    # Convert text into tokenized form\n",
    "    encoded_tokens = tokenizer.encode(text_data)\n",
    "\n",
    "    # Display the number of tokens\n",
    "    print(\"Total number of encoded tokens:\", len(encoded_tokens))\n",
    "else:\n",
    "    print(\"Error: Unable to fetch the file. HTTP Status Code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tokens = encoded_tokens[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence (x): [1263, 1230, 9051, 1402]\n",
      "Target sequence (y): [1230, 9051, 1402, 1230]\n"
     ]
    }
   ],
   "source": [
    "# Define the length of the input context\n",
    "window_size = 4  # Number of tokens the model looks at for prediction\n",
    "\n",
    "# Explanation:\n",
    "# - The model processes a sequence of 4 tokens to predict the next token.\n",
    "# - `input_tokens` represents the first 4 tokens.\n",
    "# - `target_tokens` represents the next 4 tokens shifted by one position.\n",
    "\n",
    "input_tokens = encoded_tokens[:window_size]  # First 4 tokens as input\n",
    "target_tokens = encoded_tokens[1:window_size+1]  # Next 4 tokens as target\n",
    "\n",
    "# Display the results\n",
    "print(f\"Input sequence (x): {input_tokens}\")\n",
    "print(f\"Target sequence (y): {target_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1263] ----> 1230\n",
      "[1263, 1230] ----> 9051\n",
      "[1263, 1230, 9051] ----> 1402\n",
      "[1263, 1230, 9051, 1402] ----> 1230\n"
     ]
    }
   ],
   "source": [
    "# Iterate over different input lengths from 1 to window_size\n",
    "for i in range(1, window_size + 1):\n",
    "    # Extract the context (previous i tokens)\n",
    "    context_tokens = encoded_tokens[:i]\n",
    "    # The desired target token (next token in sequence)\n",
    "    target_token = encoded_tokens[i]\n",
    "\n",
    "    # Display the context-to-target mapping\n",
    "    print(context_tokens, \"---->\", target_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " big ---->  government\n",
      " big government ---->  versus\n",
      " big government versus ---->  small\n",
      " big government versus small ---->  government\n"
     ]
    }
   ],
   "source": [
    "# Iterate over different input lengths from 1 to window_size\n",
    "for i in range(1, window_size + 1):\n",
    "    # Extract the context (previous i tokens)\n",
    "    context_tokens = encoded_tokens[:i]\n",
    "    # The desired target token (next token in sequence)\n",
    "    target_token = encoded_tokens[i]\n",
    "\n",
    "    # Decode token IDs back into text for readability\n",
    "    context_text = tokenizer.decode(context_tokens)\n",
    "    target_text = tokenizer.decode([target_token])\n",
    "\n",
    "    # Display the mapping between input sequence and expected output\n",
    "    print(context_text, \"---->\", target_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mh3511\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextSequenceDataset(Dataset):\n",
    "    def __init__(self, text_data, tokenizer, sequence_length, step_size):\n",
    "        self.input_sequences = []\n",
    "        self.target_sequences = []\n",
    "\n",
    "        # Convert the text into token IDs\n",
    "        tokenized_ids = tokenizer.encode(text_data, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Apply a sliding window approach to generate overlapping sequences\n",
    "        for start_idx in range(0, len(tokenized_ids) - sequence_length, step_size):\n",
    "            input_segment = tokenized_ids[start_idx:start_idx + sequence_length]\n",
    "            target_segment = tokenized_ids[start_idx + 1:start_idx + sequence_length + 1]\n",
    "            self.input_sequences.append(torch.tensor(input_segment))\n",
    "            self.target_sequences.append(torch.tensor(target_segment))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_sequences[index], self.target_sequences[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(text_data, batch_size=4, sequence_length=256, \n",
    "                     step_size=128, shuffle=True, drop_last=True, \n",
    "                     num_workers=0):\n",
    "\n",
    "    # Load the tokenizer (GPT-2 encoding)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset instance\n",
    "    dataset = TextSequenceDataset(text_data, tokenizer, sequence_length, step_size)\n",
    "\n",
    "    # Configure and return the DataLoader\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data retrieved successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# Request the file content from the given URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    text_content = response.text  # Extract text data from the response\n",
    "    print(\"Text data retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"Error: Unable to retrieve the file. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed PyTorch version: 2.6.0+cpu\n",
      "[tensor([[23569,   287,  4506, 22064]]), tensor([[  287,  4506, 22064,  4903]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Display the PyTorch version\n",
    "print(\"Installed PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Build the DataLoader using the fetched text data\n",
    "dataloader = build_dataloader(\n",
    "    text_content, batch_size=1, sequence_length=4, step_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "# Create an iterator for the DataLoader\n",
    "data_iterator = iter(dataloader)\n",
    "\n",
    "# Retrieve the first batch\n",
    "initial_batch = next(data_iterator)\n",
    "\n",
    "# Display the first batch\n",
    "print(initial_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  287,  4506, 22064,  4903]]), tensor([[ 4506, 22064,  4903,  3643]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iterator)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequences:\n",
      " tensor([[23569,   287,  4506, 22064],\n",
      "        [ 4903,  3643, 20518,  1122],\n",
      "        [13593,   262,  9758,  9831],\n",
      "        [  287,  5206,  8273,  1596],\n",
      "        [ 5774,  9793,   262,  2106],\n",
      "        [  286,   262, 16503,  2585],\n",
      "        [  468,   587,   281,  6306],\n",
      "        [  287,  7996,   329,   517]])\n",
      "\n",
      "Target Sequences:\n",
      " tensor([[  287,  4506, 22064,  4903],\n",
      "        [ 3643, 20518,  1122, 13593],\n",
      "        [  262,  9758,  9831,   287],\n",
      "        [ 5206,  8273,  1596,  5774],\n",
      "        [ 9793,   262,  2106,   286],\n",
      "        [  262, 16503,  2585,   468],\n",
      "        [  587,   281,  6306,   287],\n",
      "        [ 7996,   329,   517,   621]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DataLoader with specified parameters\n",
    "dataloader = build_dataloader(text_content, batch_size=8, sequence_length=4, step_size=4, shuffle=False)\n",
    "\n",
    "# Create an iterator for the DataLoader\n",
    "data_iterator = iter(dataloader)\n",
    "\n",
    "# Retrieve the first batch of inputs and targets\n",
    "input_sequences, target_sequences = next(data_iterator)\n",
    "\n",
    "# Display the input and target sequences\n",
    "print(\"Input Sequences:\\n\", input_sequences)\n",
    "print(\"\\nTarget Sequences:\\n\", target_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING TOKEN EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary size and embedding dimension\n",
    "num_tokens = 6\n",
    "embedding_dim = 3\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Initialize an embedding layer with specified parameters\n",
    "embedding_layer = torch.nn.Embedding(num_tokens, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens:\n",
      " tensor([[ 1212,   318,   281,  1672],\n",
      "        [ 2420,   329,  4856, 11525]])\n",
      "\n",
      "Target Tokens:\n",
      " tensor([[  318,   281,  1672,  2420],\n",
      "        [  329,  4856, 11525,    67]])\n"
     ]
    }
   ],
   "source": [
    "# Define sequence length\n",
    "sequence_length = 4\n",
    "\n",
    "# Initialize DataLoader with specified parameters\n",
    "dataloader = build_dataloader(\n",
    "    text_content, batch_size=2, sequence_length=4, step_size=4, shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# Check if the dataset is empty before proceeding\n",
    "if len(dataloader.dataset) == 0:\n",
    "    raise ValueError(\"Dataset is empty. Ensure the text content is long enough.\")\n",
    "\n",
    "# Create an iterator for the DataLoader\n",
    "data_iterator = iter(dataloader)\n",
    "\n",
    "# Retrieve the first batch of inputs and targets\n",
    "try:\n",
    "    input_tokens, target_tokens = next(data_iterator)\n",
    "    \n",
    "    # Display the tokenized inputs and targets\n",
    "    print(\"Input Tokens:\\n\", input_tokens)\n",
    "    print(\"\\nTarget Tokens:\\n\", target_tokens)\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"No data available in the DataLoader. Try increasing text length or reducing batch size/sequence length.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(input_tokens)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define context length based on sequence length\n",
    "context_size = sequence_length\n",
    "\n",
    "# Initialize position embedding layer\n",
    "positional_embedding_layer = torch.nn.Embedding(context_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# Generate position indices\n",
    "position_indices = torch.arange(sequence_length)\n",
    "\n",
    "# Compute position embeddings\n",
    "positional_embeddings = positional_embedding_layer(position_indices)\n",
    "\n",
    "# Display the shape of the position embeddings\n",
    "print(positional_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Input Embeddings Shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define sequence length and embedding dimensions\n",
    "sequence_length = 4  # Make sure this matches max_length\n",
    "embedding_dim = 256  # Ensure this is consistent\n",
    "\n",
    "# Initialize Token Embedding Layer (Assuming vocab_size=50257)\n",
    "token_embedding_layer = torch.nn.Embedding(50257, embedding_dim)\n",
    "\n",
    "# Generate some dummy token IDs (batch_size=8, sequence_length=4)\n",
    "batch_size = 8\n",
    "dummy_token_ids = torch.randint(0, 50257, (batch_size, sequence_length))\n",
    "\n",
    "# Compute token embeddings\n",
    "token_embeddings = token_embedding_layer(dummy_token_ids)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "# Ensure position embeddings use the correct sequence length and embedding dimension\n",
    "positional_embedding_layer = torch.nn.Embedding(sequence_length, embedding_dim)  # Ensure correct embedding size\n",
    "\n",
    "# Generate position indices and obtain positional embeddings\n",
    "position_indices = torch.arange(sequence_length).unsqueeze(0).expand(batch_size, -1)  # Shape: (batch_size, sequence_length)\n",
    "positional_embeddings = positional_embedding_layer(position_indices)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "# Verify shape alignment\n",
    "assert token_embeddings.shape == positional_embeddings.shape, \\\n",
    "    f\"Shape mismatch! Token Embeddings: {token_embeddings.shape}, Positional Embeddings: {positional_embeddings.shape}\"\n",
    "\n",
    "# Compute final input embeddings by adding token and position embeddings\n",
    "combined_embeddings = token_embeddings + positional_embeddings\n",
    "\n",
    "# Display the shape of the resulting embeddings\n",
    "print(\"Final Input Embeddings Shape:\", combined_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The input_embeddings we created are the embedded input\n",
    "examples that can now be processed by the main LLM modules\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
